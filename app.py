import streamlit as st
import os
import traceback
import time
import random
from korean_utils import KoreanTextProcessor
from ui_components import UIComponents

# Mock ModelManager for testing when dependencies are not available
class MockModelManager:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.device = "cpu"
        self.model_name = "Qwen/Qwen2-7B-Instruct"
        self.is_mock = True
        
    def load_model(self, progress_callback=None):
        """Mock model loading with progress simulation"""
        try:
            if progress_callback:
                for i in range(1, 11):
                    time.sleep(0.1)  # Simulate loading time
                    if i <= 3:
                        msg = "ë””ë°”ì´ìŠ¤ ì„¤ì • ì¤‘..." if os.getenv('LANG', '').startswith('ko') else "Setting up device..."
                    elif i <= 6:
                        msg = "í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘..." if os.getenv('LANG', '').startswith('ko') else "Loading tokenizer..."
                    elif i <= 9:
                        msg = "ëª¨ë¸ ë¡œë“œ ì¤‘..." if os.getenv('LANG', '').startswith('ko') else "Loading model..."
                    else:
                        msg = "ë¡œë“œ ì™„ë£Œ!" if os.getenv('LANG', '').startswith('ko') else "Load complete!"
                    progress_callback(i, 10, msg)
            
            self.model = "mock_model"
            self.tokenizer = "mock_tokenizer"
            return True
        except Exception as e:
            print(f"Error in mock model loading: {str(e)}")
            return False
    
    def generate_text(self, prompt, max_length=200, temperature=0.7, top_p=0.9):
        """Generate mock text response"""
        time.sleep(0.5)  # Simulate processing time
        
        korean_responses = [
            "ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” HB AIì…ë‹ˆë‹¤. í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
            "ìš”ì²­í•˜ì‹  ë‚´ìš©ì— ëŒ€í•´ ë„ì›€ì„ ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ë” êµ¬ì²´ì ì¸ ì§ˆë¬¸ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë§ì”€í•´ ì£¼ì„¸ìš”.",
            "í•œêµ­ì–´ í…ìŠ¤íŠ¸ ìƒì„± ê¸°ëŠ¥ì´ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•˜ê³  ìˆìŠµë‹ˆë‹¤. ë‹¤ì–‘í•œ ì£¼ì œì— ëŒ€í•´ ì§ˆë¬¸í•´ ë³´ì„¸ìš”.",
            "ì°½ì˜ì ì¸ ì•„ì´ë””ì–´ë¥¼ ì œê³µí•˜ê±°ë‚˜ í•™ìŠµì— ë„ì›€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì–´ë–¤ ë„ì›€ì´ í•„ìš”í•˜ì‹ ê°€ìš”?"
        ]
        
        english_responses = [
            "Hello! I'm HB AI. I can communicate naturally in both Korean and English.",
            "I'd be happy to help you with your request. Please feel free to ask more specific questions.",
            "The Korean text generation feature is working properly. You can ask me about various topics.",
            "I can provide creative ideas or help with learning. What kind of assistance do you need?"
        ]
        
        # Detect if prompt is in Korean
        if any(ord(char) >= 0xAC00 and ord(char) <= 0xD7A3 for char in prompt):
            return random.choice(korean_responses)
        else:
            return random.choice(english_responses)
    
    def generate_chat_response(self, conversation_context, max_length=300, temperature=0.7, top_p=0.9):
        """Generate mock chat response"""
        time.sleep(0.3)  # Simulate processing time
        
        # Extract the last user message from context
        if "user\n" in conversation_context:
            last_message = conversation_context.split("user\n")[-1].split("<|im_end|>")[0].strip()
        else:
            last_message = "Hello"
        
        korean_chat_responses = [
            f"'{last_message}'ì— ëŒ€í•œ í¥ë¯¸ë¡œìš´ ì§ˆë¬¸ì´ë„¤ìš”! ìì„¸íˆ ì„¤ëª…í•´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤.",
            f"ë§ì”€í•˜ì‹  '{last_message}' ê´€ë ¨í•´ì„œ ë„ì›€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì–´ë–¤ ë¶€ë¶„ì´ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?",
            "ë„¤, ì´í•´í–ˆìŠµë‹ˆë‹¤. í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”í•˜ë©° ë„ì›€ì„ ë“œë¦¬ê² ìŠµë‹ˆë‹¤.",
            "ì¢‹ì€ ì§ˆë¬¸ì…ë‹ˆë‹¤! ë” ìì„¸í•œ ì •ë³´ë¥¼ ì›í•˜ì‹œë©´ ì–¸ì œë“  ë§ì”€í•´ ì£¼ì„¸ìš”."
        ]
        
        english_chat_responses = [
            f"That's an interesting question about '{last_message}'! Let me explain in detail.",
            f"I can help you with '{last_message}'. What specific aspect would you like to know more about?",
            "Yes, I understand. I'm here to help you with natural conversation in both languages.",
            "Great question! Feel free to ask if you need more detailed information."
        ]
        
        # Detect if message is in Korean
        if any(ord(char) >= 0xAC00 and ord(char) <= 0xD7A3 for char in last_message):
            return random.choice(korean_chat_responses)
        else:
            return random.choice(english_chat_responses)
    
    def cleanup(self):
        """Mock cleanup"""
        self.model = None
        self.tokenizer = None
    
    def is_loaded(self):
        """Check if mock model is loaded"""
        return self.model is not None
    
    def get_model_info(self):
        """Get mock model information"""
        return {
            "model_name": self.model_name,
            "device": self.device,
            "loaded": self.is_loaded(),
            "cuda_available": False,
            "memory_usage": 0
        }

# Try to import real ModelManager, prefer HuggingFace API, fall back to mock if dependencies missing
try:
    from huggingface_model_manager import HuggingFaceModelManager
    ModelManager = HuggingFaceModelManager
    USE_REAL_MODEL = True
    MODEL_TYPE = "HuggingFace API"
except ImportError:
    try:
        from model_manager import ModelManager
        USE_REAL_MODEL = True
        MODEL_TYPE = "Local Model"
    except ImportError:
        ModelManager = MockModelManager
        USE_REAL_MODEL = False
        MODEL_TYPE = "Mock Model"

# Page configuration
st.set_page_config(
    page_title="HB AI - í•œêµ­ì–´ AI ì±„íŒ…",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Initialize session state
if 'language' not in st.session_state:
    st.session_state.language = 'ko'
if 'chat_history' not in st.session_state:
    st.session_state.chat_history = []
if 'model_loaded' not in st.session_state:
    st.session_state.model_loaded = False
if 'model_manager' not in st.session_state:
    st.session_state.model_manager = None

# Initialize components
ui = UIComponents()
korean_processor = KoreanTextProcessor()

def main():
    # Header with language toggle
    col1, col2, col3 = st.columns([3, 1, 1])
    
    with col1:
        if st.session_state.language == 'ko':
            st.title("ğŸ¤– HB AI - í•œêµ­ì–´ AI ì±„íŒ… ì‹œìŠ¤í…œ")
            if not USE_REAL_MODEL:
                st.markdown("**Qwen2 7B ëª¨ë¸ì„ í™œìš©í•œ í•œêµ­ì–´ í…ìŠ¤íŠ¸ ìƒì„± ë° ëŒ€í™” AI** *(í…ŒìŠ¤íŠ¸ ëª¨ë“œ)*")
                st.info("ğŸ”§ í˜„ì¬ ëª¨ì˜ ì‘ë‹µìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ì¤‘ì…ë‹ˆë‹¤.")
            else:
                st.markdown(f"**Qwen2 7B ëª¨ë¸ì„ í™œìš©í•œ í•œêµ­ì–´ í…ìŠ¤íŠ¸ ìƒì„± ë° ëŒ€í™” AI** *({MODEL_TYPE})*")
                if MODEL_TYPE == "HuggingFace API":
                    openrouter_key = os.getenv('OPENROUTER_API_KEY')
                    if openrouter_key:
                        st.success("âœ… OpenRouter API í‚¤ê°€ ì„¤ì •ë˜ì–´ ì‹¤ì œ Qwen2 7B ì‘ë‹µì„ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                        if st.session_state.model_loaded and st.session_state.model_manager:
                            model_info = st.session_state.model_manager.get_model_info()
                            st.info(f"ğŸ¤– í™œì„± ëª¨ë¸: {model_info['model_name']}")
                    else:
                        st.warning("âš ï¸ ì‹¤ì œ Qwen2 7B ì‘ë‹µì„ ìœ„í•´ OPENROUTER_API_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
        else:
            st.title("ğŸ¤– HB AI - Korean AI Chat System")
            if not USE_REAL_MODEL:
                st.markdown("**Korean text generation and conversational AI powered by Qwen2 7B** *(Testing Mode)*")
                st.info("ğŸ”§ Currently testing with mock responses.")
            else:
                st.markdown(f"**Korean text generation and conversational AI powered by Qwen2 7B** *({MODEL_TYPE})*")
                if MODEL_TYPE == "HuggingFace API":
                    openrouter_key = os.getenv('OPENROUTER_API_KEY')
                    if openrouter_key:
                        st.success("âœ… OpenRouter API key configured for real Qwen2 7B responses.")
                        if st.session_state.model_loaded and st.session_state.model_manager:
                            model_info = st.session_state.model_manager.get_model_info()
                            st.info(f"ğŸ¤– Active Model: {model_info['model_name']}")
                    else:
                        st.warning("âš ï¸ Set OPENROUTER_API_KEY for real Qwen2 7B responses via OpenRouter.")
    
    with col3:
        if st.button("ğŸŒ í•œêµ­ì–´" if st.session_state.language == 'en' else "ğŸŒ English"):
            st.session_state.language = 'ko' if st.session_state.language == 'en' else 'en'
            st.rerun()

    # Sidebar for model management
    with st.sidebar:
        ui.render_sidebar()
        
        # Model loading section
        if not st.session_state.model_loaded:
            if st.session_state.language == 'ko':
                st.header("ğŸ”§ ëª¨ë¸ ë¡œë”©")
                load_button_text = "Qwen2 7B ëª¨ë¸ ë¡œë“œ"
                loading_text = "ëª¨ë¸ì„ ë¡œë“œí•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."
                if not os.getenv('OPENROUTER_API_KEY'):
                    st.info("ğŸ’¡ OPENROUTER_API_KEY ì„¤ì •ìœ¼ë¡œ ì‹¤ì œ Qwen2 7B ì‘ë‹µì„ ë°›ìœ¼ì„¸ìš”")
            else:
                st.header("ğŸ”§ Model Loading")
                load_button_text = "Load Qwen2 7B Model"
                loading_text = "Loading model..."
                if not os.getenv('OPENROUTER_API_KEY'):
                    st.info("ğŸ’¡ Set OPENROUTER_API_KEY for real Qwen2 7B responses")
            
            if st.button(load_button_text, type="primary"):
                load_model()
        else:
            if st.session_state.language == 'ko':
                st.success("âœ… ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")
                if st.button("ğŸ”„ ëª¨ë¸ ì¬ë¡œë“œ"):
                    reload_model()
            else:
                st.success("âœ… Model loaded successfully!")
                if st.button("ğŸ”„ Reload Model"):
                    reload_model()

    # Main content area - Always show tabs, use mock responses if model not loaded
    if st.session_state.language == 'ko':
        tab1, tab2 = st.tabs(["ğŸ’¬ ì±„íŒ…", "ğŸ“ í…ìŠ¤íŠ¸ ìƒì„±"])
    else:
        tab1, tab2 = st.tabs(["ğŸ’¬ Chat", "ğŸ“ Text Generation"])
    
    with tab1:
        render_chat_interface()
    
    with tab2:
        render_text_generation()
    
    # Show welcome info if model not loaded
    if not (st.session_state.model_loaded and st.session_state.model_manager):
        with st.expander("ğŸ“– ì‹œì‘í•˜ê¸°" if st.session_state.language == 'ko' else "ğŸ“– Getting Started", expanded=True):
            ui.render_welcome_screen()

def load_model():
    """Load the Qwen2 7B model with progress indicators"""
    try:
        with st.spinner(
            "ëª¨ë¸ì„ ë¡œë“œí•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤... ì²˜ìŒ ì‹¤í–‰ì‹œ ëª‡ ë¶„ì´ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤." 
            if st.session_state.language == 'ko' 
            else "Loading model... This may take a few minutes on first run."
        ):
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            # Initialize model manager
            st.session_state.model_manager = ModelManager()
            
            # Load model with progress updates
            success = st.session_state.model_manager.load_model(
                progress_callback=lambda step, total, msg: update_progress(
                    progress_bar, status_text, step, total, msg
                )
            )
            
            if success:
                st.session_state.model_loaded = True
                progress_bar.progress(1.0)
                status_text.success(
                    "ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!" if st.session_state.language == 'ko' else "Model loaded successfully!"
                )
                st.rerun()
            else:
                error_msg = (
                    "ëª¨ë¸ ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì¸í„°ë„· ì—°ê²°ì„ í™•ì¸í•˜ê³  ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”." 
                    if st.session_state.language == 'ko' 
                    else "Failed to load model. Please check your internet connection and try again."
                )
                st.error(error_msg)
                
    except Exception as e:
        error_msg = (
            f"ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}" 
            if st.session_state.language == 'ko' 
            else f"Error loading model: {str(e)}"
        )
        st.error(error_msg)
        st.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")

def reload_model():
    """Reload the model"""
    st.session_state.model_loaded = False
    st.session_state.model_manager = None
    st.rerun()

def update_progress(progress_bar, status_text, step, total, message):
    """Update progress indicators"""
    progress = step / total if total > 0 else 0
    progress_bar.progress(progress)
    status_text.text(message)

def render_chat_interface():
    """Render the chat interface"""
    if st.session_state.language == 'ko':
        st.subheader("ğŸ’¬ AIì™€ ëŒ€í™”í•˜ê¸°")
        placeholder_text = "ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”..."
        send_button_text = "ì „ì†¡"
    else:
        st.subheader("ğŸ’¬ Chat with AI")
        placeholder_text = "Type your message..."
        send_button_text = "Send"
    
    # Display chat history
    chat_container = st.container()
    with chat_container:
        for i, (role, message) in enumerate(st.session_state.chat_history):
            if role == "user":
                st.markdown(f"**ğŸ‘¤ ì‚¬ìš©ì:** {message}" if st.session_state.language == 'ko' else f"**ğŸ‘¤ You:** {message}")
            else:
                st.markdown(f"**ğŸ¤– HB AI:** {message}")
            st.markdown("---")
    
    # Chat input
    col1, col2 = st.columns([4, 1])
    with col1:
        user_input = st.text_input(
            label="chat_input",
            placeholder=placeholder_text,
            label_visibility="collapsed",
            key="chat_input"
        )
    with col2:
        send_button = st.button(send_button_text, type="primary", use_container_width=True)
    
    # Process chat input
    if send_button and user_input.strip():
        process_chat_message(user_input.strip())

def render_text_generation():
    """Render the text generation interface"""
    if st.session_state.language == 'ko':
        st.subheader("ğŸ“ í…ìŠ¤íŠ¸ ìƒì„±")
        st.markdown("í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•˜ì—¬ AIê°€ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ë„ë¡ í•˜ì„¸ìš”.")
    else:
        st.subheader("ğŸ“ Text Generation")
        st.markdown("Enter a prompt to generate text with AI.")
    
    # Generation parameters
    col1, col2, col3 = st.columns(3)
    with col1:
        max_length = st.slider(
            "ìµœëŒ€ ê¸¸ì´" if st.session_state.language == 'ko' else "Max Length", 
            min_value=50, max_value=1000, value=200
        )
    with col2:
        temperature = st.slider(
            "ì°½ì˜ì„±" if st.session_state.language == 'ko' else "Creativity", 
            min_value=0.1, max_value=2.0, value=0.7, step=0.1
        )
    with col3:
        top_p = st.slider(
            "ë‹¤ì–‘ì„±" if st.session_state.language == 'ko' else "Diversity", 
            min_value=0.1, max_value=1.0, value=0.9, step=0.1
        )
    
    # Text input
    prompt = st.text_area(
        "í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”:" if st.session_state.language == 'ko' else "Enter your prompt:",
        height=100,
        placeholder="ì˜ˆ: í•œêµ­ì˜ ì „í†µ ìŒì‹ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”." if st.session_state.language == 'ko' else "e.g., Tell me about Korean traditional food."
    )
    
    # Generate button
    if st.button(
        "í…ìŠ¤íŠ¸ ìƒì„±" if st.session_state.language == 'ko' else "Generate Text", 
        type="primary"
    ):
        if prompt.strip():
            generate_text(prompt.strip(), max_length, temperature, top_p)
        else:
            st.warning(
                "í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”." if st.session_state.language == 'ko' else "Please enter a prompt."
            )

def process_chat_message(user_input):
    """Process chat message and generate AI response"""
    try:
        # Add user message to history
        st.session_state.chat_history.append(("user", user_input))
        
        # Generate AI response
        with st.spinner(
            "AIê°€ ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..." if st.session_state.language == 'ko' else "AI is generating response..."
        ):
            response = None
            
            # Try real model first if available
            if st.session_state.model_loaded and st.session_state.model_manager:
                try:
                    # Prepare conversation context
                    conversation_context = korean_processor.prepare_chat_context(st.session_state.chat_history)
                    
                    # Generate response
                    response = st.session_state.model_manager.generate_chat_response(
                        conversation_context,
                        max_length=300,
                        temperature=0.7,
                        top_p=0.9
                    )
                except Exception as e:
                    print(f"Real model failed: {str(e)}")
                    response = None
            
            # Fall back to mock responses if real model unavailable or failed
            if not response:
                print("Using mock response fallback")
                mock_manager = MockModelManager()
                conversation_context = korean_processor.prepare_chat_context(st.session_state.chat_history)
                response = mock_manager.generate_chat_response(conversation_context)
            
            if response:
                # Process Korean text
                processed_response = korean_processor.post_process_response(response)
                st.session_state.chat_history.append(("assistant", processed_response))
            else:
                error_msg = (
                    "ì‘ë‹µ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”." 
                    if st.session_state.language == 'ko' 
                    else "Failed to generate response. Please try again."
                )
                st.session_state.chat_history.append(("assistant", error_msg))
        
        # Clear input and refresh
        st.rerun()
        
    except Exception as e:
        error_msg = (
            f"ì±„íŒ… ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}" 
            if st.session_state.language == 'ko' 
            else f"Error processing chat: {str(e)}"
        )
        st.error(error_msg)

def generate_text(prompt, max_length, temperature, top_p):
    """Generate text based on prompt"""
    try:
        with st.spinner(
            "í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..." if st.session_state.language == 'ko' else "Generating text..."
        ):
            generated_text = None
            
            # Try real model first if available
            if st.session_state.model_loaded and st.session_state.model_manager:
                try:
                    # Prepare prompt for Korean processing
                    processed_prompt = korean_processor.prepare_generation_prompt(prompt)
                    
                    # Generate text
                    generated_text = st.session_state.model_manager.generate_text(
                        processed_prompt,
                        max_length=max_length,
                        temperature=temperature,
                        top_p=top_p
                    )
                except Exception as e:
                    print(f"Real model failed: {str(e)}")
                    generated_text = None
            
            # Fall back to mock responses if real model unavailable or failed
            if not generated_text:
                print("Using mock text generation fallback")
                mock_manager = MockModelManager()
                generated_text = mock_manager.generate_text(prompt, max_length, temperature, top_p)
            
            if generated_text:
                # Post-process Korean text
                final_text = korean_processor.post_process_response(generated_text)
                
                st.subheader(
                    "ìƒì„±ëœ í…ìŠ¤íŠ¸:" if st.session_state.language == 'ko' else "Generated Text:"
                )
                st.markdown(f"**í”„ë¡¬í”„íŠ¸:** {prompt}" if st.session_state.language == 'ko' else f"**Prompt:** {prompt}")
                st.markdown("**ì‘ë‹µ:**" if st.session_state.language == 'ko' else "**Response:**")
                st.write(final_text)
                
                # Copy button
                st.code(final_text, language=None)
            else:
                st.error(
                    "í…ìŠ¤íŠ¸ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤." if st.session_state.language == 'ko' else "Failed to generate text."
                )
                
    except Exception as e:
        error_msg = (
            f"í…ìŠ¤íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}" 
            if st.session_state.language == 'ko' 
            else f"Error generating text: {str(e)}"
        )
        st.error(error_msg)

if __name__ == "__main__":
    main()
